{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1099f61-e70f-4939-83f7-af9cc0a3fed8",
   "metadata": {},
   "source": [
    "# Data Quality and Data Wrangling \n",
    "## Course Code: DLBDSDQDW01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bc8316-7198-415b-aeea-78849b4f7e9a",
   "metadata": {},
   "source": [
    "## Task 2: Scrape the web"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a41436-7851-4d39-929f-fa79c948859c",
   "metadata": {},
   "source": [
    "This notebook describes the implementation of Task 2 of the Data Quality and Data Wrangling course (DLBDSDQDW01), it contains the code use for experimentation and the creation of the visualization according to the requirements in the task description."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36800db-9cb6-4bb1-925d-6d15b136cc99",
   "metadata": {},
   "source": [
    "### Data sources\n",
    "The data was collected from the following sources:\n",
    "\n",
    "1. [OpenWeather](https://openweathermap.org/api/one-call-3#concept): for weather data such as temperature, humidity, pressure, etc.\n",
    "2. [AlphaVantage](https://www.alphavantage.co/documentation/): for Stock market data\n",
    "2. [USGS Earthquake](https://earthquake.usgs.gov/fdsnws/event/1/): earthquake data for the world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb07882-e3b7-48da-ad2a-226265cca545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# used packages\n",
    "import requests\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import h5py\n",
    "import re\n",
    "import time\n",
    "import folium\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "from datetime import date, datetime, timezone, timedelta\n",
    "from zoneinfo import ZoneInfo\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f67cd3a-bdae-48d2-9de0-cfe1b096aaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the API keys\n",
    "load_dotenv(Path().cwd().parent.joinpath(r\"config/.env\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ff84c6-03ad-45f4-b768-9dbd85c567b6",
   "metadata": {},
   "source": [
    "# Define location for time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15188c08-c23e-4860-b65b-c5f9fc6ef9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = [\"Berlin\" , \"Munich\", \"Hamburg\", \"Baden_Baden\", \"Paris\", \"Strasbourg\", \"Madrid\", \"Malaga\", \"Barcelona\", \"Lisbon\", \"Mexico_city\", \"Monterrey\", \"Tokio\", \"Osaka\", \"Kyoto\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d61bbd-55c4-488a-a4a3-882bb8bc362a",
   "metadata": {},
   "source": [
    "# Weather data\n",
    "\n",
    "Since the API provides data only for one timestamp and not the summary for the whole day, the request will be run every 2 hours, to get 12 readings per day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8871f0-c12a-408e-aa3a-b45b833f0cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENWEATHER_API_KEY = os.getenv(\"OPENWEATHER_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9944ed6e-6bdd-4dfb-86d3-fbe2fb8269a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch(url, headers:dict=None, params:dict=None):\n",
    "    response = requests.get(url, params=params)\n",
    "    response.raise_for_status()\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc264940-bd63-4281-8738-b20c891416eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# geocoding endpoint - to get coordinates of the cities\n",
    "url_geocoding = \"http://api.openweathermap.org/geo/1.0/direct?\"\n",
    "params_geocoding = {\"q\":None, \"limit\":1, \"appid\" : OPENWEATHER_API_KEY}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27aa58bf-fae0-4b87-8b79-b955dfd3aa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_cities = []\n",
    "for city in cities:\n",
    "    params_geocoding[\"q\"] = city\n",
    "    response = fetch(url_geocoding, params=params_geocoding)\n",
    "    responses_cities.append(response)\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a01265-ef2e-4cce-b3bf-8c1e6090dc4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "responses_cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffea5f4e-ae1c-4be9-9bbc-09d5a093983e",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_coordinates = {\"city\" : [], \"country\" : [], \"lon\" : [], \"lat\" : []}\n",
    "for response_list in responses_cities: # a response list is the response of the API, a list containing a dictionary\n",
    "    for response in response_list: # response is the dictionary containing the information of the countries\n",
    "        city_coordinates[\"city\"].append(response[\"name\"])\n",
    "        city_coordinates[\"lon\"].append(response[\"lon\"]) # X\n",
    "        city_coordinates[\"lat\"].append(response[\"lat\"]) # Y\n",
    "        city_coordinates[\"country\"].append(response[\"country\"])\n",
    "df_geolocations = pd.DataFrame(city_coordinates)\n",
    "#df_geolocations[\"geometry\"] = df_geolocations.apply(lambda row: Point(row[\"lon\"], row[\"lat\"]), axis=1)\n",
    "#df_geolocations = gpd.GeoDataFrame(df_geolocations, geometry=\"geometry\", crs=\"EPSG:4326\")\n",
    "df_geolocations.to_csv(\"geocoding_openweather.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002ff836-ba80-49b0-bb7d-409c684b5849",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geolocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807af323-bad1-4e2e-ae3e-58ad0d541e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geolocations = df_geolocations.replace(dict(zip(df_geolocations.city.tolist(), cities)))\n",
    "df_geolocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd83a6a-6257-4a04-9a50-8dd31bd09fbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_geolocations.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae7ea29-1b29-44f5-929f-61af75e177ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weather endpoint - to get weather of the locations\n",
    "url_weather = \"https://pro.openweathermap.org/data/2.5/weather\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fd56c4-a908-491f-845e-7da8ab86ad94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time zone for Berlin\n",
    "berlin = ZoneInfo(\"Europe/Berlin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c871595-6b91-43ea-ab70-d6f48c95bd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.now(berlin).replace(microsecond=0)\n",
    "\n",
    "responses_weather = []\n",
    "\n",
    "for nrow, record in df_geolocations.iterrows():\n",
    "    lon = record[\"lon\"]\n",
    "    lat = record[\"lat\"]\n",
    "    params = {\"units\":\"metric\",\n",
    "              \"lon\" : lon,\n",
    "              \"lat\" : lat,\n",
    "              \"date\" : today.isoformat(),\n",
    "              \"appid\" : OPENWEATHER_API_KEY}\n",
    "    response = requests.get(url_weather, params)\n",
    "    response.raise_for_status()\n",
    "    responses_weather.append(response)\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55bf07c-3632-409f-a856-4848cf9cacef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How does the response look like?\n",
    "responses_weather[0].json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88283f61-f779-429b-9352-dbad0b619133",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = {\"name\" : [], # city name - it might not match no automatic geocoding by the API\n",
    "           \"temperature\" : [], # Temperature\n",
    "           \"temperature_max\" : [], # Max temp at the moment\n",
    "           \"temperature_min\" : [], # Min temp at the moment\n",
    "           \"feels_like\" : [], # Human perception of the weather\n",
    "           \"humidity\":[], #\n",
    "           \"wind_speed\":[], # in m/\n",
    "           \"wind_direction\" : [],\n",
    "           \"description\":[],\n",
    "           \"timestamp\":[]}\n",
    "\n",
    "for response in responses_weather:\n",
    "    weather_data = response.json()\n",
    "    weather[\"name\"].append(weather_data[\"name\"])\n",
    "    weather[\"temperature\"].append(weather_data[\"main\"][\"temp\"])\n",
    "    weather[\"temperature_max\"].append(weather_data[\"main\"][\"temp_max\"])\n",
    "    weather[\"temperature_min\"].append(weather_data[\"main\"][\"temp_min\"])\n",
    "    weather[\"feels_like\"].append(weather_data[\"main\"][\"feels_like\"])\n",
    "    weather[\"humidity\"].append(weather_data[\"main\"][\"humidity\"])\n",
    "    weather[\"wind_speed\"].append(weather_data[\"wind\"][\"speed\"])\n",
    "    weather[\"wind_direction\"].append(weather_data[\"wind\"][\"deg\"])\n",
    "    weather[\"description\"].append(weather_data[\"weather\"][0][\"description\"])\n",
    "    weather[\"timestamp\"].append(pd.to_datetime(weather_data[\"dt\"], unit=\"s\"))\n",
    "    \n",
    "df_weather = pd.DataFrame(weather)\n",
    "df_weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eebd147-065d-4655-ab9f-a484fc6c82fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# commbine into one dataframe for the weather data\n",
    "df_weather = pd.concat([df_geolocations, df_weather], axis=1)\n",
    "df_weather = df_weather.drop(\"name\", axis=1)\n",
    "df_weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2d9549-5773-4e13-960d-00e1c9a25598",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8352bf-1c8b-42ac-9c81-9e7a1152ab8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather[\"split_on\"] = df_weather[\"city\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b4398a-0e51-4bc3-92b2-8a52357f707c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10edd32-f23e-4934-861f-d123a4b9f24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if weather data exists load it, if not\n",
    "if Path(\"weather_data.csv\").exists():\n",
    "    print(\"loading latest data\")\n",
    "    history_df = pd.read_csv(\"weather_data.csv\")\n",
    "    # this dataframe is the final weather data. Store in staging area to combine later with further data\n",
    "    df_weather = pd.concat([history_df, df_weather], axis=0).sort_values(by=[\"city\", \"timestamp\"], ascending=False)\n",
    "    df_weather.to_csv(\"weather_data.csv\", index=False)\n",
    "else:\n",
    "    print(\"weather_data.csv does not exist. Latest data will be stored\")\n",
    "    df_weather.to_csv(\"weather_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3f76a5-790d-4f02-9e6a-605052110988",
   "metadata": {},
   "source": [
    "# Stock market data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8863c11c-8ab4-4659-b376-b6d6af9e600b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHAVANTAGE_API_KEY = os.getenv(\"ALPHAVANTAGE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7abf9a-f3c8-4306-9c01-5a9b12b34dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tickers = (\"NVDA\",\"AAPL\")#,\"META\",\"RHM.DE\",\"SPY\",\"URTH\",\"ACWI\")\n",
    "tickers = (\"META\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e23dd0-4cf8-4e25-b294-6cc54fe46d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176b5c42-fb3d-4f9f-9f7d-44d9a0d8f9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_stocks = \"https://www.alphavantage.co/query\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0234c66b-64c7-4da6-b564-9af2decd617e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to build the time series, two cases are identified.\n",
    "# 1) The first time the VantageClient collects data from the API and 2) with data already exists for the specific symbols\n",
    "\n",
    "# Does the dataStore already exit?\n",
    "stocks = []\n",
    "if Path(\"/Users/jorgetellez/Documents/06_Projects/IU_Data_Wrangling/data/processed/datastore.h5\").exists():\n",
    "    store = pd.HDFStore(\"/Users/jorgetellez/Documents/06_Projects/IU_Data_Wrangling/data/processed/datastore.h5\", \"r\")\n",
    "    print(\"DataStore content\", store.keys())\n",
    "    keys = [key.split(r\"/\")[-1] for key in store.keys() if key.split(r\"/\")[1] == \"stocks\" and key.split(r\"/\")[2] == \"data\"]\n",
    "    store.close()\n",
    "    print(keys)\n",
    "    for symbol in tickers:\n",
    "        params_stocks = {\"function\" : \"TIME_SERIES_DAILY\", # this endpoint provides a daily time series of the equity specified\n",
    "                     \"symbol\" : symbol, # the equity -> replace this by all the companies that should be followed\n",
    "                     \"outputsize\" : \"compact\",\n",
    "                     \"dataype\" : \"json\",\n",
    "                     \"apikey\" : ALPHAVANTAGE_API_KEY}\n",
    "        response = requests.get(url_stocks, params=params_stocks)\n",
    "        response.raise_for_status()\n",
    "        stock = response.json()[\"Time Series (Daily)\"]\n",
    "        stocks.append({symbol:stock})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2afdb6-1760-4067-96ac-2bbc68a3873e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9255515-c1ce-4158-bb12-9bb908b99276",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(r\"/Users/jorgetellez/Documents/06_Projects/IU_Data_Wrangling/data/raw/stocks/20250806_174026_stocks.json\") as s:\n",
    "    stocks = json.load(s)\n",
    "    print(stocks)\n",
    "    print(type(stocks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074cf51f-79c5-43ae-9eca-0ee81d14af90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stocks[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee72d0f-3ece-4040-a6ed-03d6d8be7399",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for stock in stocks:\n",
    "    for symbol, data in stock.items():\n",
    "        df = pd.DataFrame(data).T\n",
    "        df = df.set_index(pd.to_datetime(df.index))\n",
    "        df = df.apply(pd.to_numeric, errors=\"coerce\")\n",
    "        df = df.rename(columns={\"1. open\" :\"open\", \"2. high\":\"high\", \"3. low\":\"low\", \"4. close\":\"close\", \"5. volume\": \"volume\"})\n",
    "        df[\"symbol\"] = symbol\n",
    "        df[\"split_on\"] = df[\"symbol\"]\n",
    "        if symbol in keys:\n",
    "            df = pd.DataFrame(df.iloc[0, :]).T\n",
    "            dfs.append(df)\n",
    "            print(symbol, \"exists\")\n",
    "        else:\n",
    "            dfs.append(df)\n",
    "df_stocks = pd.concat(dfs, axis=0)\n",
    "#df_stocks.loc[:,[\"open\", \"high\", \"low\", \"close\", \"volume\"]] = df_stocks.loc[:,[\"open\", \"high\", \"low\", \"close\", \"volume\"]].apply(pd.to_numeric, errors=\"coerce\")\n",
    "df_stocks[[\"open\", \"high\", \"low\", \"close\", \"volume\"]] = df_stocks[[\"open\", \"high\", \"low\", \"close\", \"volume\"]].astype(\"float64\")\n",
    "df_stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31fb5e4-a479-47ea-8539-76f851a85797",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stocks.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb1033c-2d09-479f-916c-0b528c61d1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(24,6), nrows=1, ncols=len(tickers)+1, sharex=True)\n",
    "for idx, ticker in enumerate(tickers):\n",
    "    df = df_stocks[df_stocks[\"symbol\"] == ticker]\n",
    "    ax[idx].set_title(ticker)\n",
    "    ax[idx].plot(df[\"open\"], marker='.', label=\"Open\")\n",
    "    ax[idx].plot(df[\"close\"], marker='.', label=\"Close\")\n",
    "    #ax[idx].set_ylim([0,250])\n",
    "    ax[idx].grid(True, axis=\"y\", linestyle=\":\", linewidth=0.5)\n",
    "    ax[idx].spines[[\"top\", \"right\", \"bottom\", \"left\"]].set_visible(False)\n",
    "    ax[idx].legend(loc=\"upper right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da7f4ef-7a3d-431e-96d1-8c6a4c3becb4",
   "metadata": {},
   "source": [
    "# Earthquake data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fd89f0-a851-450a-b833-a00c5c175058",
   "metadata": {},
   "source": [
    "Parameters according the API documentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8572ee65-725d-414a-b8cc-ea9f573261b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL\n",
    "url = r\"https://earthquake.usgs.gov/fdsnws/event/1/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb3559c-62e4-49cc-9535-9bb9bf599b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# According to the API documentation all times use UTC\n",
    "# Time in Germany should be specified when making a request\n",
    "berlin = ZoneInfo(\"Europe/Berlin\")\n",
    "tokyo = ZoneInfo(\"Asia/Tokyo\")\n",
    "now = datetime.now(berlin).replace(microsecond=0)\n",
    "yesterday = now - timedelta(days=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c710a99c-7462-4a30-a117-77b4f6997b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query parameters\n",
    "params = {\"method\" : \"query\", # submit a data request\n",
    "          \"format\" : \"geojson\", # reponse format\n",
    "          #\"minlatitude\" : 24.0, # Get earthquakes in Japan\n",
    "          #\"maxlatitude\" : 46.0,\n",
    "          #\"minlongitude\" : 122.0,\n",
    "          #\"maxlongitude\" : 146,\n",
    "          \"limit\" : 100, # Limit results to this value\n",
    "          \"starttime\": yesterday.isoformat(), # the API expect ISO time format, here it is set\n",
    "          \"endtime\" : now.isoformat(),\n",
    "          \"orderby\" : \"time\"} # sort the results from most recent to oldest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee58d06-cfe6-4f90-9d4a-49ba7121c258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data from the API\n",
    "response = requests.get(url, params=params)\n",
    "earthquakes = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f6d712-aad6-48e8-91b2-551155a89977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same names of the reponse in the dict for easier iteration\n",
    "records = {\"time\":[], # time when the event ocurred - in milliseconds since the epoch\n",
    "           \"mag\":[], # magnitude of the event - combine with magType for interpretation\n",
    "           \"magType\":[], # magnitude types are described in the API documentation - must be mapped to a name easier to understand\n",
    "           \"alert\":[],\n",
    "           \"tsunami\":[],\n",
    "           \"place\":[],\n",
    "           \"coordinates\":[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c48de4-82f0-4c62-8897-ac7c77c6c02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for earthquake in earthquakes[\"features\"]:\n",
    "    for feature in records:\n",
    "        if feature in earthquake[\"properties\"]:\n",
    "            records[feature].append(earthquake[\"properties\"][feature])\n",
    "        else:\n",
    "            # geometry is a key in the response\n",
    "            records[feature].append(earthquake[\"geometry\"][feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d009e0d-d3a8-4f50-8d60-306a42ad3345",
   "metadata": {},
   "outputs": [],
   "source": [
    "mag_type_description = {\n",
    "    \"Mw\": \"Moment Magnitude\",\n",
    "    \"Ms\": \"Surface Wave Magnitude\",\n",
    "    \"mb\": \"Body Wave Magnitude\",\n",
    "    \"ml\": \"Local (Richter) Magnitude\",\n",
    "    \"mb_lg\": \"Lg-Wave Magnitude\",\n",
    "    \"md\": \"Duration Magnitude\",\n",
    "    \"MH\": \"Hand-calculated Magnitude\",\n",
    "    \"MI\": \"Intensity-derived Magnitude\",\n",
    "    \"Me\": \"Energy Magnitude\",\n",
    "    \"Mg\": \"Surface Wave from Ground Displacement\",\n",
    "    \"MWb\": \"Moment Magnitude from Body Waves\",\n",
    "    \"Mwr\": \"Regional Moment Magnitude\",\n",
    "    \"MwC\": \"Centroid Moment Magnitude\",\n",
    "    \"MwB\": \"Body-wave Derived Moment Magnitude\",\n",
    "    \"mww\": \"Moment Magnitude from W-phase\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5606fe59-2431-47ef-8991-6c6692c0d5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: set alert to black in case no alert\n",
    "df = pd.DataFrame(records)\n",
    "#df[\"geometry\"] = df.coordinates.apply(lambda coord: Point(coord[:2]))\n",
    "df[\"lon\"] = df.coordinates.apply(lambda coord: coord[0])\n",
    "df[\"lat\"] = df.coordinates.apply(lambda coord: coord[1])\n",
    "df[\"depth\"] = df.coordinates.apply(lambda coord: coord[2])\n",
    "df = df.rename(columns={\"time\":\"timestamp\", \"mag\":\"magnitude\", \"magType\":\"scale\"})\n",
    "#df[\"magType\"] = df[\"magType\"].str.lower()\n",
    "df = df.drop(\"coordinates\", axis=1)\n",
    "\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], unit=\"ms\")\n",
    "df[\"split_on\"] = df[\"timestamp\"].dt.strftime(\"date_%Y_%m_%d\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0168ff89-3a51-4fad-bce1-9baf3459de1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "geodf = gpd.GeoDataFrame(df, geometry=\"geometry\", crs=\"EPSG:4326\")\n",
    "geodf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bc5ae8-b077-4364-9d24-59a7c8ec4da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "geodf.explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce55a76f-642b-41ea-b68e-35754419a77f",
   "metadata": {},
   "source": [
    "# Data Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b5f103-a81e-4595-a225-b1466c5d3d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store(client:str, df:pd.DataFrame, split_on:str):\n",
    "    # Make sure there is a column on which it can be grouped on\n",
    "    if split_on not in df.columns:\n",
    "        raise ValueError(f\"{split_on} not found in Dataframe columns: {df.columns}\")\n",
    "    \n",
    "    # to avoid min_size problems when storing string columns\n",
    "    for col in df.select_dtypes(include=[\"object\", \"string\"]).columns:\n",
    "        entry_len = df[col].astype(str).map(len).max()  \n",
    "        min_itemsize = max(entry_len, 15) # if the column is empty, use this as the default length\n",
    "\n",
    "    with pd.HDFStore(\"/Users/jorgetellez/Documents/06_Projects/IU_Data_Wrangling/data/processed/datastore.h5\", \"a\") as datastore:\n",
    "        for group, data in df.groupby(split_on):\n",
    "            datastore.append(\n",
    "                f\"/{client}/data/{group}\",\n",
    "                data,\n",
    "                format=\"table\",\n",
    "                data_columns=True,\n",
    "                min_itemsize=min_itemsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2a96ca-2ef0-49f3-b932-56afd27ccbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "store(\"weather\", df_weather, \"split_on\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffd0743-27aa-4cf2-ae9e-c6e103e7659f",
   "metadata": {},
   "outputs": [],
   "source": [
    "store(\"earthquake\", df, \"split_on\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd66393d-a6cf-40e2-b858-c64514309bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "store(\"stocks\", df_stocks, \"split_on\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae30700e-bb73-4889-8f8c-dc2d2ec1908c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.HDFStore(\"/Users/jorgetellez/Documents/06_Projects/IU_Data_Wrangling/data/processed/datastore.h5\", \"r\") as ds:\n",
    "    for key in ds.keys():\n",
    "        display(key)\n",
    "        display(ds[key])\n",
    "        display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856115fe-5d3d-4d99-8fc9-44e066160a2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8eebd6-171b-4120-b2dc-7f3b5f1eaf5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
