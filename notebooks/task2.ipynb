{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1099f61-e70f-4939-83f7-af9cc0a3fed8",
   "metadata": {},
   "source": [
    "# Data Quality and Data Wrangling \n",
    "## Course Code: DLBDSDQDW01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bc8316-7198-415b-aeea-78849b4f7e9a",
   "metadata": {},
   "source": [
    "## Task 2: Scrape the web"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a41436-7851-4d39-929f-fa79c948859c",
   "metadata": {},
   "source": [
    "This notebook describes the implementation of Task 2 of the Data Quality and Data Wrangling course (DLBDSDQDW01), it contains the code use for experimentation and the creation of the visualization according to the requirements in the task description."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36800db-9cb6-4bb1-925d-6d15b136cc99",
   "metadata": {},
   "source": [
    "### Data sources\n",
    "The data was collected from the following sources:\n",
    "\n",
    "1. [OpenWeather](https://openweathermap.org/api/one-call-3#concept): for weather data such as temperature, humidity, pressure, etc.\n",
    "2. [AlphaVantage](https://www.alphavantage.co/documentation/): for Stock market data\n",
    "2. [USGS Earthquake](https://earthquake.usgs.gov/fdsnws/event/1/): earthquake data for the world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb07882-e3b7-48da-ad2a-226265cca545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# used packages\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import geodatasets\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "from datetime import date, datetime, timezone, timedelta\n",
    "from zoneinfo import ZoneInfo\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f67cd3a-bdae-48d2-9de0-cfe1b096aaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the API keys\n",
    "load_dotenv(Path().cwd().parent.joinpath(r\"config/.env\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ff84c6-03ad-45f4-b768-9dbd85c567b6",
   "metadata": {},
   "source": [
    "# Define location for time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15188c08-c23e-4860-b65b-c5f9fc6ef9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = [\"Berlin\" , \"Munich\", \"Hamburg\", \"Baden_Baden\", \"Paris\", \"Madrid\", \"Mexico_city\", \"Tokio\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d61bbd-55c4-488a-a4a3-882bb8bc362a",
   "metadata": {},
   "source": [
    "# Weather data\n",
    "\n",
    "Since the API provides data only for one timestamp and not the summary for the whole day, the request will be run every 2 hours, to get 12 readings per day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8871f0-c12a-408e-aa3a-b45b833f0cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENWEATHER_API_KEY = os.getenv(\"OPENWEATHER_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9944ed6e-6bdd-4dfb-86d3-fbe2fb8269a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch(url, headers:dict=None, params:dict=None):\n",
    "    response = requests.get(url, params=params)\n",
    "    response.raise_for_status()\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc264940-bd63-4281-8738-b20c891416eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# geocoding endpoint - to get coordinates of the cities\n",
    "url_geocoding = \"http://api.openweathermap.org/geo/1.0/direct?\"\n",
    "params_geocoding = {\"q\":None, \"limit\":1, \"appid\" : OPENWEATHER_API_KEY}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27aa58bf-fae0-4b87-8b79-b955dfd3aa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_cities = []\n",
    "for city in cities:\n",
    "    params_geocoding[\"q\"] = city\n",
    "    response = fetch(url_geocoding, params=params_geocoding)\n",
    "    responses_cities.append(response)\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a01265-ef2e-4cce-b3bf-8c1e6090dc4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "responses_cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffea5f4e-ae1c-4be9-9bbc-09d5a093983e",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_coordinates = {\"city\" : [], \"country\" : [], \"lon\" : [], \"lat\" : []}\n",
    "for response_list in responses_cities: # a response list is the response of the API, a list containing a dictionary\n",
    "    for response in response_list: # response is the dictionary containing the information of the countries\n",
    "        city_coordinates[\"city\"].append(response[\"name\"])\n",
    "        city_coordinates[\"lon\"].append(response[\"lon\"]) # X\n",
    "        city_coordinates[\"lat\"].append(response[\"lat\"]) # Y\n",
    "        city_coordinates[\"country\"].append(response[\"country\"])\n",
    "df_geolocations = pd.DataFrame(city_coordinates)\n",
    "#df_geolocations[\"geometry\"] = df_geolocations.apply(lambda row: Point(row[\"lon\"], row[\"lat\"]), axis=1)\n",
    "#df_geolocations = gpd.GeoDataFrame(df_geolocations, geometry=\"geometry\", crs=\"EPSG:4326\")\n",
    "df_geolocations.to_csv(\"geocoding_openweather.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002ff836-ba80-49b0-bb7d-409c684b5849",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geolocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807af323-bad1-4e2e-ae3e-58ad0d541e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geolocations = df_geolocations.replace(dict(zip(df_geolocations.city.tolist(), cities)))\n",
    "df_geolocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd83a6a-6257-4a04-9a50-8dd31bd09fbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_geolocations.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae7ea29-1b29-44f5-929f-61af75e177ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weather endpoint - to get weather of the locations\n",
    "url_weather = \"https://pro.openweathermap.org/data/2.5/weather\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fd56c4-a908-491f-845e-7da8ab86ad94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time zone for Berlin\n",
    "berlin = ZoneInfo(\"Europe/Berlin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c871595-6b91-43ea-ab70-d6f48c95bd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.now(berlin).replace(microsecond=0)\n",
    "\n",
    "responses_weather = []\n",
    "\n",
    "for nrow, record in df_geolocations.iterrows():\n",
    "    lon = record[\"lon\"]\n",
    "    lat = record[\"lat\"]\n",
    "    params = {\"units\":\"metric\",\n",
    "              \"lon\" : lon,\n",
    "              \"lat\" : lat,\n",
    "              \"date\" : today.isoformat(),\n",
    "              \"appid\" : OPENWEATHER_API_KEY}\n",
    "    response = requests.get(url_weather, params)\n",
    "    response.raise_for_status()\n",
    "    responses_weather.append(response)\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55bf07c-3632-409f-a856-4848cf9cacef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How does the response look like?\n",
    "responses_weather[0].json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88283f61-f779-429b-9352-dbad0b619133",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = {\"name\" : [], # city name - it might not match no automatic geocoding by the API\n",
    "           \"temperature\" : [], # Temperature\n",
    "           \"temperature_max\" : [], # Max temp at the moment\n",
    "           \"temperature_min\" : [], # Min temp at the moment\n",
    "           \"feels_like\" : [], # Human perception of the weather\n",
    "           \"humidity\":[], #\n",
    "           \"wind_speed\":[], # in m/\n",
    "           \"wind_direction\" : [],\n",
    "           \"description\":[],\n",
    "           \"timestamp\":[]}\n",
    "\n",
    "for response in responses_weather:\n",
    "    weather_data = response.json()\n",
    "    weather[\"name\"].append(weather_data[\"name\"])\n",
    "    weather[\"temperature\"].append(weather_data[\"main\"][\"temp\"])\n",
    "    weather[\"temperature_max\"].append(weather_data[\"main\"][\"temp_max\"])\n",
    "    weather[\"temperature_min\"].append(weather_data[\"main\"][\"temp_min\"])\n",
    "    weather[\"feels_like\"].append(weather_data[\"main\"][\"feels_like\"])\n",
    "    weather[\"humidity\"].append(weather_data[\"main\"][\"humidity\"])\n",
    "    weather[\"wind_speed\"].append(weather_data[\"wind\"][\"speed\"])\n",
    "    weather[\"wind_direction\"].append(weather_data[\"wind\"][\"deg\"])\n",
    "    weather[\"description\"].append(weather_data[\"weather\"][0][\"description\"])\n",
    "    weather[\"timestamp\"].append(pd.to_datetime(weather_data[\"dt\"], unit=\"s\"))\n",
    "    \n",
    "df_weather = pd.DataFrame(weather)\n",
    "df_weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eebd147-065d-4655-ab9f-a484fc6c82fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# commbine into one dataframe for the weather data\n",
    "df_weather = pd.concat([df_geolocations, df_weather], axis=1)\n",
    "df_weather = df_weather.drop(\"name\", axis=1)\n",
    "df_weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2d9549-5773-4e13-960d-00e1c9a25598",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8352bf-1c8b-42ac-9c81-9e7a1152ab8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather[\"split_on\"] = df_weather[\"city\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b4398a-0e51-4bc3-92b2-8a52357f707c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10edd32-f23e-4934-861f-d123a4b9f24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if weather data exists load it, if not\n",
    "if Path(\"weather_data.csv\").exists():\n",
    "    print(\"loading latest data\")\n",
    "    history_df = pd.read_csv(\"weather_data.csv\")\n",
    "    # this dataframe is the final weather data. Store in staging area to combine later with further data\n",
    "    df_weather = pd.concat([history_df, df_weather], axis=0).sort_values(by=[\"city\", \"timestamp\"], ascending=False)\n",
    "    df_weather.to_csv(\"weather_data.csv\", index=False)\n",
    "else:\n",
    "    print(\"weather_data.csv does not exist. Latest data will be stored\")\n",
    "    df_weather.to_csv(\"weather_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3f76a5-790d-4f02-9e6a-605052110988",
   "metadata": {},
   "source": [
    "# Stock market data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8863c11c-8ab4-4659-b376-b6d6af9e600b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHAVANTAGE_API_KEY = os.getenv(\"ALPHAVANTAGE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7abf9a-f3c8-4306-9c01-5a9b12b34dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tickers = (\"NVDA\",\"AAPL\")#,\"META\",\"RHM.DE\",\"SPY\",\"URTH\",\"ACWI\")\n",
    "tickers = (\"META\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e23dd0-4cf8-4e25-b294-6cc54fe46d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176b5c42-fb3d-4f9f-9f7d-44d9a0d8f9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_stocks = \"https://www.alphavantage.co/query\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0234c66b-64c7-4da6-b564-9af2decd617e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to build the time series, two cases are identified.\n",
    "# 1) The first time the VantageClient collects data from the API and 2) with data already exists for the specific symbols\n",
    "\n",
    "# Does the dataStore already exit?\n",
    "stocks = []\n",
    "if Path(\"/Users/jorgetellez/Documents/06_Projects/IU_Data_Wrangling/data/processed/datastore.h5\").exists():\n",
    "    store = pd.HDFStore(\"/Users/jorgetellez/Documents/06_Projects/IU_Data_Wrangling/data/processed/datastore.h5\", \"r\")\n",
    "    print(\"DataStore content\", store.keys())\n",
    "    keys = [key.split(r\"/\")[-1] for key in store.keys() if key.split(r\"/\")[1] == \"stocks\" and key.split(r\"/\")[2] == \"data\"]\n",
    "    store.close()\n",
    "    print(keys)\n",
    "    for symbol in tickers:\n",
    "        params_stocks = {\"function\" : \"TIME_SERIES_DAILY\", # this endpoint provides a daily time series of the equity specified\n",
    "                     \"symbol\" : symbol, # the equity -> replace this by all the companies that should be followed\n",
    "                     \"outputsize\" : \"compact\",\n",
    "                     \"dataype\" : \"json\",\n",
    "                     \"apikey\" : ALPHAVANTAGE_API_KEY}\n",
    "        response = requests.get(url_stocks, params=params_stocks)\n",
    "        response.raise_for_status()\n",
    "        stock = response.json()[\"Time Series (Daily)\"]\n",
    "        stocks.append({symbol:stock})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2afdb6-1760-4067-96ac-2bbc68a3873e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9255515-c1ce-4158-bb12-9bb908b99276",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(r\"/Users/jorgetellez/Documents/06_Projects/IU_Data_Wrangling/data/raw/stocks/20250806_174026_stocks.json\") as s:\n",
    "    stocks = json.load(s)\n",
    "    print(stocks)\n",
    "    print(type(stocks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074cf51f-79c5-43ae-9eca-0ee81d14af90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stocks[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee72d0f-3ece-4040-a6ed-03d6d8be7399",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for stock in stocks:\n",
    "    for symbol, data in stock.items():\n",
    "        df = pd.DataFrame(data).T\n",
    "        df = df.set_index(pd.to_datetime(df.index))\n",
    "        df = df.apply(pd.to_numeric, errors=\"coerce\")\n",
    "        df = df.rename(columns={\"1. open\" :\"open\", \"2. high\":\"high\", \"3. low\":\"low\", \"4. close\":\"close\", \"5. volume\": \"volume\"})\n",
    "        df[\"symbol\"] = symbol\n",
    "        df[\"split_on\"] = df[\"symbol\"]\n",
    "        if symbol in keys:\n",
    "            df = pd.DataFrame(df.iloc[0, :]).T\n",
    "            dfs.append(df)\n",
    "            print(symbol, \"exists\")\n",
    "        else:\n",
    "            dfs.append(df)\n",
    "df_stocks = pd.concat(dfs, axis=0)\n",
    "#df_stocks.loc[:,[\"open\", \"high\", \"low\", \"close\", \"volume\"]] = df_stocks.loc[:,[\"open\", \"high\", \"low\", \"close\", \"volume\"]].apply(pd.to_numeric, errors=\"coerce\")\n",
    "df_stocks[[\"open\", \"high\", \"low\", \"close\", \"volume\"]] = df_stocks[[\"open\", \"high\", \"low\", \"close\", \"volume\"]].astype(\"float64\")\n",
    "df_stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31fb5e4-a479-47ea-8539-76f851a85797",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stocks.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb1033c-2d09-479f-916c-0b528c61d1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(24,6), nrows=1, ncols=len(tickers)+1, sharex=True)\n",
    "for idx, ticker in enumerate(tickers):\n",
    "    df = df_stocks[df_stocks[\"symbol\"] == ticker]\n",
    "    ax[idx].set_title(ticker)\n",
    "    ax[idx].plot(df[\"open\"], marker='.', label=\"Open\")\n",
    "    ax[idx].plot(df[\"close\"], marker='.', label=\"Close\")\n",
    "    #ax[idx].set_ylim([0,250])\n",
    "    ax[idx].grid(True, axis=\"y\", linestyle=\":\", linewidth=0.5)\n",
    "    ax[idx].spines[[\"top\", \"right\", \"bottom\", \"left\"]].set_visible(False)\n",
    "    ax[idx].legend(loc=\"upper right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da7f4ef-7a3d-431e-96d1-8c6a4c3becb4",
   "metadata": {},
   "source": [
    "# Earthquake data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fd89f0-a851-450a-b833-a00c5c175058",
   "metadata": {},
   "source": [
    "Parameters according the API documentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8572ee65-725d-414a-b8cc-ea9f573261b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL\n",
    "url = r\"https://earthquake.usgs.gov/fdsnws/event/1/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb3559c-62e4-49cc-9535-9bb9bf599b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# According to the API documentation all times use UTC\n",
    "# Time in Germany should be specified when making a request\n",
    "berlin = ZoneInfo(\"Europe/Berlin\")\n",
    "tokyo = ZoneInfo(\"Asia/Tokyo\")\n",
    "now = datetime.now(berlin).replace(microsecond=0)\n",
    "yesterday = now - timedelta(days=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c710a99c-7462-4a30-a117-77b4f6997b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query parameters\n",
    "params = {\"method\" : \"query\", # submit a data request\n",
    "          \"format\" : \"geojson\", # reponse format\n",
    "          #\"minlatitude\" : 24.0, # Get earthquakes in Japan\n",
    "          #\"maxlatitude\" : 46.0,\n",
    "          #\"minlongitude\" : 122.0,\n",
    "          #\"maxlongitude\" : 146,\n",
    "          \"limit\" : 100, # Limit results to this value\n",
    "          \"starttime\": yesterday.isoformat(), # the API expect ISO time format, here it is set\n",
    "          \"endtime\" : now.isoformat(),\n",
    "          \"orderby\" : \"time\"} # sort the results from most recent to oldest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee58d06-cfe6-4f90-9d4a-49ba7121c258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data from the API\n",
    "response = requests.get(url, params=params)\n",
    "earthquakes = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f6d712-aad6-48e8-91b2-551155a89977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same names of the reponse in the dict for easier iteration\n",
    "records = {\"time\":[], # time when the event ocurred - in milliseconds since the epoch\n",
    "           \"mag\":[], # magnitude of the event - combine with magType for interpretation\n",
    "           \"magType\":[], # magnitude types are described in the API documentation - must be mapped to a name easier to understand\n",
    "           \"alert\":[],\n",
    "           \"tsunami\":[],\n",
    "           \"place\":[],\n",
    "           \"coordinates\":[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c48de4-82f0-4c62-8897-ac7c77c6c02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for earthquake in earthquakes[\"features\"]:\n",
    "    for feature in records:\n",
    "        if feature in earthquake[\"properties\"]:\n",
    "            records[feature].append(earthquake[\"properties\"][feature])\n",
    "        else:\n",
    "            # geometry is a key in the response\n",
    "            records[feature].append(earthquake[\"geometry\"][feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d009e0d-d3a8-4f50-8d60-306a42ad3345",
   "metadata": {},
   "outputs": [],
   "source": [
    "mag_type_description = {\n",
    "    \"Mw\": \"Moment Magnitude\",\n",
    "    \"Ms\": \"Surface Wave Magnitude\",\n",
    "    \"mb\": \"Body Wave Magnitude\",\n",
    "    \"ml\": \"Local (Richter) Magnitude\",\n",
    "    \"mb_lg\": \"Lg-Wave Magnitude\",\n",
    "    \"md\": \"Duration Magnitude\",\n",
    "    \"MH\": \"Hand-calculated Magnitude\",\n",
    "    \"MI\": \"Intensity-derived Magnitude\",\n",
    "    \"Me\": \"Energy Magnitude\",\n",
    "    \"Mg\": \"Surface Wave from Ground Displacement\",\n",
    "    \"MWb\": \"Moment Magnitude from Body Waves\",\n",
    "    \"Mwr\": \"Regional Moment Magnitude\",\n",
    "    \"MwC\": \"Centroid Moment Magnitude\",\n",
    "    \"MwB\": \"Body-wave Derived Moment Magnitude\",\n",
    "    \"mww\": \"Moment Magnitude from W-phase\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5606fe59-2431-47ef-8991-6c6692c0d5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(records)\n",
    "df[\"lon\"] = df.coordinates.apply(lambda coord: coord[0])\n",
    "df[\"lat\"] = df.coordinates.apply(lambda coord: coord[1])\n",
    "df[\"depth\"] = df.coordinates.apply(lambda coord: coord[2])\n",
    "df = df.rename(columns={\"time\":\"timestamp\", \"mag\":\"magnitude\", \"magType\":\"scale\"})\n",
    "df = df.drop(\"coordinates\", axis=1)\n",
    "\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], unit=\"ms\")\n",
    "df[\"split_on\"] = df[\"timestamp\"].dt.strftime(\"date_%Y_%m_%d\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0168ff89-3a51-4fad-bce1-9baf3459de1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "geodf = gpd.GeoDataFrame(df, geometry=\"geometry\", crs=\"EPSG:4326\")\n",
    "geodf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bc5ae8-b077-4364-9d24-59a7c8ec4da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "geodf.explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce55a76f-642b-41ea-b68e-35754419a77f",
   "metadata": {},
   "source": [
    "# Data Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b5f103-a81e-4595-a225-b1466c5d3d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store(client, df, split_on):\n",
    "    # make sure there is a column on which it can be grouped on\n",
    "    if split_on not in df.columns:\n",
    "        raise ValueError(f\"{split_on} not found in Dataframe columns: {df.columns}\")\n",
    "    \n",
    "    # to avoid min_size problems when storing string columns\n",
    "    for col in df.select_dtypes(include=[\"object\", \"string\"]).columns:\n",
    "        entry_len = df[col].astype(str).map(len).max()  \n",
    "        min_itemsize = max(entry_len, 15) # if the column is empty, use this as the default length\n",
    "\n",
    "    datastore = Path().cwd().joinpath(r\"datastore_dev.h5\")\n",
    "    with pd.HDFStore(datastore, \"a\") as datastore:\n",
    "        for group, data in df.groupby(split_on):\n",
    "            datastore.append(\n",
    "                f\"/{client}/data/{group}\",\n",
    "                data,\n",
    "                format=\"table\",\n",
    "                data_columns=True,\n",
    "                min_itemsize=min_itemsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2a96ca-2ef0-49f3-b932-56afd27ccbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "store(\"weather\", df_weather, \"split_on\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffd0743-27aa-4cf2-ae9e-c6e103e7659f",
   "metadata": {},
   "outputs": [],
   "source": [
    "store(\"earthquake\", df, \"split_on\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd66393d-a6cf-40e2-b858-c64514309bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "store(\"stocks\", df_stocks, \"split_on\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae30700e-bb73-4889-8f8c-dc2d2ec1908c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gett all the data\n",
    "datastore = Path().cwd().parents[0].joinpath(r\"data/processed/datastore.h5\")\n",
    "weather = []\n",
    "stocks = []\n",
    "earthquakes = []\n",
    "with pd.HDFStore(datastore, \"r\") as ds:\n",
    "    for key in ds.keys():\n",
    "        key_structure = key.split(\"/\")\n",
    "        if key_structure[1] == \"weather\" and key_structure[2] == \"data\":\n",
    "            weather.append((key_structure[-1], ds[key].set_index(\"timestamp\")))\n",
    "        elif key_structure[1] == \"stocks\" and key_structure[2] == \"data\":\n",
    "            stocks.append((key_structure[-1], ds[key]))\n",
    "        elif key_structure[1] == \"earthquake\" and key_structure[2] == \"data\":\n",
    "            earthquakes.append((key_structure[-1], ds[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8eebd6-171b-4120-b2dc-7f3b5f1eaf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create figure\n",
    "fig, axs = plt.subplots(figsize=(20,18), nrows=len(weather)//2, ncols=2, sharex=False, sharey=True) # use figsize=(20,12) for report\n",
    "temp_color = '#d62728'    # red for temperature\n",
    "humidity_color = '#2ca02c'  # green for humidity\n",
    "grid_color = '#e0e0e0' # gray for grid\n",
    "\n",
    "for ax, weather_data in zip(axs.flatten(), weather):\n",
    "    # extract the city name and the data\n",
    "    city, data = weather_data\n",
    "    # use a second y-axis for the humidity\n",
    "    ax2 = ax.twinx()\n",
    "    # plot the data\n",
    "    ax.plot(data[\"temperature\"], color=temp_color, alpha=0.9, label=\"Temperature\")\n",
    "    ax2.plot(data[\"humidity\"], color=humidity_color, alpha=0.9, label=\"humidity\")\n",
    "    # remove the spines for each axis\n",
    "    ax.spines[[\"top\", \"right\"]].set_visible(False)\n",
    "    ax2.spines[[\"top\", \"left\"]].set_visible(False)\n",
    "\n",
    "    # limits for the axis\n",
    "    ax.set_ylim([0,40])\n",
    "    ax2.set_ylim([0,100])\n",
    "    \n",
    "    # y-axis of the same color, for better readability \n",
    "    ax.spines['left'].set_color(temp_color)\n",
    "    ax.spines['left'].set_linewidth(1.5)\n",
    "    ax.spines['bottom'].set_color('gray')\n",
    "    ax.spines['bottom'].set_linewidth(0.8)\n",
    "    \n",
    "    ax2.spines['right'].set_color(humidity_color)\n",
    "    ax2.spines['right'].set_linewidth(1.5)\n",
    "\n",
    "    # format the coordinate systems\n",
    "    ax.set_title(city.replace(\"_\", \" \"), fontsize=15, fontweight=\"normal\", pad=15) #  some cities have a \"_\" in the name, remove it\n",
    "    ax.set_ylabel(\"Temperature [°C]\", color=temp_color, fontsize=14, fontweight='normal')\n",
    "    ax2.set_ylabel(\"Humidity [%]\", color=humidity_color, fontsize=14, fontweight='normal')\n",
    "    \n",
    "    ax.tick_params(axis='y', labelcolor=temp_color, colors=temp_color)\n",
    "    ax2.tick_params(axis='y', labelcolor=humidity_color, colors=humidity_color)\n",
    "    \n",
    "    # x-axis must be formatted\n",
    "    # make sure the DataFrames have a datetime index!\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%m.%d %H:%M'))\n",
    "    ax.xaxis.set_major_locator(mdates.HourLocator(interval=12))\n",
    "    ax.xaxis.set_minor_locator(mdates.HourLocator(interval=6))\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right', fontsize=12)\n",
    "    plt.setp(ax.yaxis.get_majorticklabels(), fontsize=12)\n",
    "    plt.setp(ax2.yaxis.get_majorticklabels(), fontsize=12)\n",
    "\n",
    "    ax.grid(True, linestyle='-', linewidth=0.3, color=grid_color, which=\"major\", alpha=1)\n",
    "    ax.set_axisbelow(True)\n",
    "\n",
    "    # remove the ticks\n",
    "    ax.tick_params(axis='both', which=\"minor\", length=0, pad=8, labelsize=10)\n",
    "    ax2.tick_params(axis='both', which=\"minor\", length=0, pad=8, labelsize=10)\n",
    "\n",
    "# uncomment when creating image for word\n",
    "fig.suptitle(\"Weather data by city\", fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.savefig(Path().cwd().parents[0].joinpath(\"utils/img/weather_plots.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f983c576-542c-4530-ae34-a16e16572e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "quakes = [data for date, data in earthquakes]\n",
    "quakes = pd.concat(quakes, ignore_index=True, axis=0)\n",
    "quakes[\"geometry\"] = quakes.apply(lambda row: Point(row[\"lon\"], row[\"lat\"]), axis=1)\n",
    "quakes = gpd.GeoDataFrame(quakes, geometry=\"geometry\", crs=\"EPSG:4326\")\n",
    "quakes = quakes.to_crs(\"EPSG:3857\") # mercator projection\n",
    "quakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b508603-b271-42b4-b11f-a86dec630b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create map - use mercator projection instead of plate carre\n",
    "fig, ax = plt.subplots(figsize=(20,20), subplot_kw={\"projection\":ccrs.Mercator()})\n",
    "ax.add_feature(cfeature.COASTLINE, linewidth=0.8, edgecolor='#2d3436', alpha=0.7)\n",
    "ax.add_feature(cfeature.LAND, facecolor='#ddd6c1', alpha=0.3)\n",
    "ax.add_feature(cfeature.OCEAN, color='#a8dadc', alpha=0.3)\n",
    "ax.add_feature(cfeature.BORDERS, linewidth=0.5, edgecolor='#636e72', alpha=0.5)\n",
    "# add the gridlines\n",
    "gridliner = ax.gridlines(draw_labels=True, linewidth=0.5, alpha=0.5, color='gray', linestyle=':')\n",
    "gridliner.top_labels = False\n",
    "gridliner.right_labels = False\n",
    "gridliner.xlabel_style = {\"fontsize\":14}\n",
    "gridliner.ylabel_style = {\"fontsize\":14}\n",
    "\n",
    "# format of the earthquakes\n",
    "colors = quakes[\"depth\"] # color according to the depth, the deeper the darker\n",
    "\n",
    "ax.scatter(quakes.geometry.x, quakes.geometry.y,\n",
    "           c=colors,\n",
    "           alpha=0.4)\n",
    "\n",
    "# add the colorbar to represent the depth\n",
    "sm = plt.cm.ScalarMappable(cmap=\"viridis_r\", norm=plt.Normalize(vmin=quakes[\"depth\"].min(), vmax=quakes[\"depth\"].max()))\n",
    "cbar = plt.colorbar(sm, ax=ax, shrink=0.4, aspect=30, pad=0.02)\n",
    "cbar.set_label(\"Depth (km)\", fontsize=14)\n",
    "\n",
    "ax.set_title('Earthquake Distribution by Depth (Mercator Projection)', fontsize=16, pad=20)\n",
    "fig.tight_layout()\n",
    "plt.savefig(Path().cwd().parents[0].joinpath(\"utils/img/earthquakes_plot.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5475edec-da16-4e51-94f6-db77e8dec3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols = {\"NVDA\":\"Nvidia\", \"AAPL\":\"Apple\", \"META\":\"Meta\", \"RHM.DE\":\"Rheinmetall\", \"SPY\":\"SP&500\", \"URTH\": \"MSCI World\", \"ACWI\":\"MSCI ACWI\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beac65c3-0ce8-4c35-bbb3-6a1989a69a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create figure\n",
    "fig, axs = plt.subplots(figsize=(21,14), nrows=len(stocks)//2, ncols=2, sharex=False, sharey=False)\n",
    "\n",
    "# Use the same colors as for the temperature\n",
    "downcolor = '#d62728'    # red for price decrease\n",
    "upcolor = '#2ca02c'  # green for price increase\n",
    "width_candle = 1\n",
    "width_wick = 0.5\n",
    "\n",
    "for ax, stocks_data in zip(axs.flatten(), stocks):\n",
    "    # extract the city name and the data\n",
    "    symbol, data = stocks_data\n",
    "    data = data.sort_index()\n",
    "    data = data[~data.index.duplicated()]\n",
    "    up = data[data[\"close\"] >= data[\"open\"]]\n",
    "    down = data[data[\"close\"] < data[\"open\"]]\n",
    "    ax.set_title(f\"{symbols[symbol]} ({symbol})\", fontsize=15, pad=15)\n",
    "    # increasing prices\n",
    "    ax.bar(up.index, height=up[\"close\"]-up[\"open\"], width=width_candle, bottom=up[\"open\"], color=upcolor) # body of the candle\n",
    "    ax.bar(up.index, height=up[\"high\"]-up[\"close\"], width=width_wick, bottom=up[\"close\"], color=upcolor) # upper wick of the candle\n",
    "    ax.bar(up.index, height=up[\"low\"]-up[\"open\"], width=width_wick, bottom=up[\"open\"], color=upcolor) # lower wick of the candle\n",
    "    # decreasing prices\n",
    "    ax.bar(down.index, height=down[\"close\"]-down[\"open\"], width=width_candle, bottom=down[\"open\"], color=downcolor) # body of the candle\n",
    "    ax.bar(down.index, height=down[\"high\"]-down[\"open\"], width=width_wick, bottom=down[\"open\"], color=downcolor) # upper wick of the candle\n",
    "    ax.bar(down.index, height=down[\"low\"]-down[\"close\"], width=width_wick, bottom=down[\"close\"], color=downcolor) # upper wick of the candle    \n",
    "    \n",
    "    ax.spines[[\"top\", \"right\"]].set_visible(False)\n",
    "    if symbol == \"RHM.DE\":\n",
    "        y_label = \"Stock prince (EUR)\"\n",
    "    else:\n",
    "        y_label = \"Stock price (USD)\"\n",
    "    ax.set_ylabel(y_label, fontsize=14, fontweight='normal')\n",
    "\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y.%m.%d'))\n",
    "    ax.xaxis.set_major_locator(mdates.DayLocator(interval=5))\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=90, ha='center', fontsize=12)\n",
    "    plt.setp(ax.yaxis.get_majorticklabels(), fontsize=12)\n",
    "\n",
    "    ax.grid(True, linestyle='-', linewidth=0.3, color=grid_color, axis=\"y\", which=\"major\", alpha=1)\n",
    "    \n",
    "# uncomment when creating image for word\n",
    "fig.suptitle(\"Stock prices (Last 100 days)\", fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.savefig(Path().cwd().parents[0].joinpath(\"utils/img/candlestick_plots.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f5658c-640a-4c7d-b7a6-574bd869d403",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abf2975-739f-48bd-b5eb-15482136226a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
